{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ECG Generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That notebook generates a training set for the ECG stream anomaly detection using GAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T22:29:27.504439Z",
     "end_time": "2023-08-22T22:29:35.503890Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('C:/ecg.csv', header=None)\n",
    "raw_data = dataframe.values\n",
    "\n",
    "# get last element\n",
    "labels = raw_data[:, -1]\n",
    "\n",
    "# rest are data\n",
    "data = raw_data[:, 0:-1]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=21)\n",
    "\n",
    "min_val =  tf.reduce_min(raw_data)\n",
    "max_val = tf.reduce_max(raw_data)\n",
    "\n",
    "train_data = (train_data - min_val)/ (max_val - min_val)\n",
    "\n",
    "train_data = tf.cast(train_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)\n",
    "\n",
    "train_labels = train_labels.astype(bool)\n",
    "test_labels = test_labels.astype(bool)\n",
    "\n",
    "normal_train_data = train_data[train_labels]\n",
    "normal_test_data = test_data[test_labels]\n",
    "\n",
    "anomalous_train_data = train_data[~train_labels]\n",
    "anomalous_test_data = test_data[~test_labels]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T20:40:43.169349Z",
     "end_time": "2023-08-22T20:40:43.863269Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The GAN model\n",
    "Below there is a definition of GAN model that generates ECG-like data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_generator(dataset, input_dim):\n",
    "    # Define the dimensions of the data\n",
    "    data = dataset\n",
    "\n",
    "    # Define the generator model\n",
    "    def build_generator():\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(64, input_dim=input_dim))\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(256))\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(512))\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(input_dim))\n",
    "        model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "        return model\n",
    "\n",
    "    # Define the discriminator model\n",
    "    def build_discriminator():\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(512, input_dim=input_dim))\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(256))\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(64))\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "        return model\n",
    "\n",
    "    # Build the GAN model\n",
    "    def build_gan(generator, discriminator):\n",
    "        discriminator.trainable = False\n",
    "        gan_input = tf.keras.layers.Input(shape=(input_dim,))\n",
    "        generated_data = generator(gan_input)\n",
    "        gan_output = discriminator(generated_data)\n",
    "        gan = tf.keras.models.Model(inputs=gan_input, outputs=gan_output)\n",
    "        gan.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "        return gan\n",
    "\n",
    "    # Create instances of the generator and discriminator\n",
    "    generator = build_generator()\n",
    "    discriminator = build_discriminator()\n",
    "\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "\n",
    "    gan = build_gan(generator, discriminator)\n",
    "\n",
    "    # Training loop\n",
    "    batch_size = 32\n",
    "    epochs = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Generate random noise as input for the generator\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, input_dim))\n",
    "\n",
    "        # Generate fake data using the generator\n",
    "        generated_data = generator.predict(noise)\n",
    "\n",
    "        # Get the number of data samples in your dataset\n",
    "        num_samples = data.shape[0]\n",
    "\n",
    "    # Set the number of randomly chosen data samples you want\n",
    "        num_random_samples = 10\n",
    "\n",
    "    # Randomly select indices of the data samples\n",
    "        random_indices = np.random.choice(num_samples, size=num_random_samples, replace=False)\n",
    "\n",
    "    # Retrieve the randomly chosen data samples\n",
    "        random_data = []\n",
    "\n",
    "        for i in random_indices:\n",
    "            random_data.append(data[i])\n",
    "\n",
    "    # Concatenate the real data and generated data\n",
    "        x = np.concatenate((random_data, generated_data))\n",
    "\n",
    "        # Create the labels for the discriminator\n",
    "        y = np.zeros(42)\n",
    "        y[:batch_size] = 1\n",
    "\n",
    "        # Train the discriminator\n",
    "        discriminator_loss = discriminator.train_on_batch(x, y)\n",
    "\n",
    "        # Generate new random noise as input for the generator\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, input_dim))\n",
    "\n",
    "        # Create labels for the generator (tricking the discriminator)\n",
    "        y = np.ones(batch_size)\n",
    "\n",
    "        # Train the generator (via the whole GAN model, with the discriminator weights frozen)\n",
    "        generator_loss = gan.train_on_batch(noise, y)\n",
    "\n",
    "        # Print the progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} | Discriminator loss: {discriminator_loss} | Generator loss: {generator_loss}\")\n",
    "    # Generate samples using the trained generator\n",
    "    return generator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:07:23.556036Z",
     "end_time": "2023-08-22T21:07:23.588132Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creation of generators' models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normal_generator = sample_generator(normal_train_data, 140)\n",
    "anomaly_generator= sample_generator(anomalous_train_data, 140)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:07:25.442956Z",
     "end_time": "2023-08-22T21:12:37.797788Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model exporting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normal_generator.save('../models/generators/normal_ecg_generator')\n",
    "anomaly_generator.save('../models/generators/anomalous_ecg_generator')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:16:26.072535Z",
     "end_time": "2023-08-22T21:16:28.501782Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## New data generating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normal_generator = tf.keras.models.load_model('../models/generators/normal_ecg_generator')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:42:41.483872Z",
     "end_time": "2023-08-22T21:42:43.288958Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anomaly_generator = tf.keras.models.load_model('../models/generators/anomalous_ecg_generator')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:42:43.966961Z",
     "end_time": "2023-08-22T21:42:45.450031Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normal_generated_data = normal_generator.predict(np.random.normal(0, 1, size=(7500, 140)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:42:45.976469Z",
     "end_time": "2023-08-22T21:42:49.178184Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anomalous_generated_data = anomaly_generator.predict(np.random.normal(0,1, size=(2500, 140)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:42:49.190187Z",
     "end_time": "2023-08-22T21:42:50.230518Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(normal_train_data[400:600], normal_generated_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:07:09.957628Z",
     "end_time": "2023-08-22T21:07:09.989248Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(mse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:07:10.473475Z",
     "end_time": "2023-08-22T21:07:10.525287Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data postprocessing\n",
    "The code below filters generated data to make them more 'natural'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(normal_generated_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-16T22:33:53.205710Z",
     "end_time": "2023-08-16T22:33:53.221334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_data(generated_data):\n",
    "    dataset = []\n",
    "\n",
    "    for sample in generated_data:\n",
    "        filtered_generated_sample = []\n",
    "        filtered_generated_sample = np.append(filtered_generated_sample, scipy.signal.wiener(sample, 15))\n",
    "        dataset.append(filtered_generated_sample)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:15:59.606453Z",
     "end_time": "2023-08-22T21:15:59.638141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normal_generated_data = filter_data(normal_generated_data)\n",
    "anomalous_generated_data = filter_data(anomalous_generated_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:42:55.626615Z",
     "end_time": "2023-08-22T21:42:57.489178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(normal_train_data[400:600], normal_generated_data)\n",
    "print(mse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:16:05.636340Z",
     "end_time": "2023-08-22T21:16:05.670935Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autoencoder_model = tf.keras.models.load_model('../models/detectors/autoencoder')\n",
    "ann_model = tf.keras.models.load_model('../models/detectors/ann')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data labeling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_column = np.ones(7500)\n",
    "normal_generated_data = np.concatenate((normal_generated_data, new_column[:, np.newaxis]), axis=1)\n",
    "new_column = np.zeros(2500)\n",
    "anomalous_generated_data = np.concatenate((anomalous_generated_data, new_column[:, np.newaxis]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:43:13.353183Z",
     "end_time": "2023-08-22T21:43:13.426702Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creation of training dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generated_data = np.concatenate((normal_generated_data, anomalous_generated_data), axis=0)\n",
    "np.random.shuffle(generated_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:43:23.105315Z",
     "end_time": "2023-08-22T21:43:23.147331Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training dataset exporting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('../dataset/ecg_10k.csv', 'w', encoding='UTF-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(generated_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T21:43:32.354126Z",
     "end_time": "2023-08-22T21:43:34.518374Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
